---
title: Chat Completions
description: Prediction Guard in action
slug: usingllms/chat
---

(Run this example in Google Colab
[here](https://drive.google.com/file/d/15EBnc9aSC_KWeL677b_QHEOO7hIWDn1e/view?usp=sharing))

We briefly introduced few shot chat prompts in the basic prompting tutorial. However, chat is a special scenario when it comes to LLMs because: (1) it is a very frequently occuring use case; (2) there are many models fine-tuned specifically for chat; and (3) the handling of message threads, context, and instructions in chat prompts is always the same.

As such, Prediction Guard has specifically created a "chat completions" enpoint within its API and Python client. This tutorial will demonstrate how to easy create a simple chatbot with the chat completions endpoint.

We will use Python to show an example:

## Dependencies and Imports

You will need to install Prediction Guard into your Python environment.

```bash copy
$ pip install predictionguard
```

Now import PredictionGuard, setup your API Key, and create the client.

```python copy
import os

from predictionguard import PredictionGuard

# Set your Prediction Guard token as an environmental variable.
os.environ["PREDICTIONGUARD_API_KEY"] = "<api key>"

client = PredictionGuard()
```

## Basic Chat Completion

Chat completions are enabled in the Prediction Guard API for only certain of the
models. You don't have to worry about special prompt templates when doing these
completions as they are already implemented.

You can find out more about the available [Models](options/enumerations) in the docs.

To perform a chat completion, you need to create an array of `messages`. Each
message object should have a:

- `role` - "system", "user", or "assistant"
- `content` - the text associated with the message

You can utilize a single "system" role prompt to give general instructions to the
bot. Then you should include the message memory from your chatbot in the message
array. This gives the model the relevant context from the conversation to respond
appropriately.

```python copy
messages = [
    {
        "role": "system",
        "content": "You are a helpful assistant that provide clever and sometimes funny responses."
    },
    {
        "role": "user",
        "content": "What's up!"
    },
    {
        "role": "assistant",
        "content": "Well, technically vertically out from the center of the earth."
    },
    {
        "role": "user",
        "content": "Haha. Good one."
    }
]

result = client.chat.completions.create(
    model="neural-chat-7b-v3-3",
    messages=messages
)

print(json.dumps(
    result,
    sort_keys=True,
    indent=4,
    separators=(',', ': ')
))
```

## Simple Chatbot

Here we will show the chat functionality with the most simple of chat UI, which
just asks for messages and prints the message thread. We will create an evolving
message thread and respond with the chat completion portion of the Python client
highlighted above.

```python copy
print('Welcome to the Chatbot! Let me know how can I help you')

while True:
    print('')
    request = input('User' + ': ')
    if request=="Stop" or request=='stop':
        print('Bot: Bye!')
        break
    else:
        messages.append({
            "role": "user",
            "content": request
        })

        response = client.chat.completions.create(
            model="neural-chat-7b-v3-3",
            messages=messages
        )['choices'][0]['message']['content'].split('\n')[0].strip()

        messages.append({
            "role": "assistant",
            "content": response
        })
        print('Bot: ', response)
```

## Using The SDKs

You can also try these examples using the other official SDKs:

[Python](/sdk-docs/software-development-kits/sd-ks),
[Go](/sdk-docs/software-development-kits/sd-ks),
[Rust](/sdk-docs/software-development-kits/sd-ks),
[JS](/sdk-docs/software-development-kits/sd-ks),
[HTTP](/api-reference)
