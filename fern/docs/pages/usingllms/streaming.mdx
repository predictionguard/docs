---
title: Streaming
---

(Run this example in Google Colab
[here](https://colab.research.google.com/drive/1JO2AeeOfwy0vMNRPjr1bHgO1bjzs2zrW?usp=sharing))

The Streaming API allows for real-time data transmission during the generation of
API responses. By enabling the stream option, responses are sent incrementally,
allowing users to begin processing parts of the response as they are received.
This is especially useful for applications requiring immediate partial data
rather than waiting for a complete response.

**Immediate Access**: Receive parts of the data as they are generated, which can
be useful for displaying real-time results or processing large volumes of data.

**Efficiency**: Improve the responsiveness of applications by handling data as
it arrives, which can be particularly beneficial in time-sensitive scenarios.

We will use Python to show an example:

## Dependencies and Imports

You will need to install Prediction Guard into your Python environment.

```bash copy
$ pip install predictionguard
```

Now import PredictionGuard, setup your API Key, and create the client.

```python copy
import os

from predictionguard import PredictionGuard


# Set your Prediction Guard API key and URL as environmental variables.
os.environ["PREDICTIONGUARD_API_KEY"] = "<api key>"
os.environ["PREDICTIONGUARD_URL"] = "<pg url>"

# You can also set them when initializing the client.
client = PredictionGuard(
    api_key="<Your PG API Key>",
    url="<Your PG API URL>"
)
```

## How To Use The Streaming API

To use the streaming capability, set the `stream` parameter to `True` in your
API request. Below is an example using the {{TEXT_MODEL}} model:

```python filename="main.py"

messages = [
    {
        "role": "system",
        "content": "You are a helpful assistant that provide clever and sometimes funny responses."
    },
    {
        "role": "user",
        "content": "What's up!"
    },
    {
        "role": "assistant",
        "content": "Well, technically vertically out from the center of the earth."
    },
    {
        "role": "user",
        "content": "Haha. Good one."
    }
]

for res in client.chat.completions.create(
    model="{{TEXT_MODEL}}",
    messages=messages,
    max_completion_tokens=500,
    temperature=0.1,
    stream=True
):

    # Use 'end' parameter in print function to avoid new lines.
    print(res["data"]["choices"][0]["delta"]["content"], end='')
```

![Retrieval augmented generation](streaming.gif)

## Using The SDKs

You can also try these examples using the other official SDKs:

[Python](/sdk-docs/software-development-kits/sd-ks),
[Go](/sdk-docs/software-development-kits/sd-ks),
[Rust](/sdk-docs/software-development-kits/sd-ks),
[JS](/sdk-docs/software-development-kits/sd-ks),
[HTTP](/api-reference)
