---
title: Quick Start
description: Reliable, future proof AI predictions
---

Technical teams need to figure out how to integrate the latest Large Language Models (LLMs), but:

- You can’t build robust systems with inconsistent, unvalidated outputs; and
- LLM integrations scare corporate lawyers, finance departments, and security professionals due to hallucinations, cost, lack of compliance (e.g., HIPAA), leaked IP/PII, and “injection” vulnerabilities.

Some companies are moving forward anyway by investing tons of engineering time/money in their own wrappers around LLMs and expensive hosting with OpenAI/Azure. Others are ignoring these issues and pressing forward with fragile and risky LLM integrations.

At Prediction Guard, we think that you should get useful output from compliant AI systems (without crazy implementation/ hosting costs), so our solution lets you:

1. **De-risk LLM inputs** to remove PII and prompt injections;
2. **Validate and check LLM outputs** to guard against hallucination, toxicity and inconsistencies; and
3. **Implement private and compliant LLM systems** (HIPAA and self-hosted) that give your legal counsel warm fuzzy feeling while still delighting your customers with AI features.

Sounds pretty great right? Follow the steps below to starting leveraging trustworthy LLMs:

## 1. Get access to Prediction Guard Enterprise

We host and control the latest LLMs for you in our secure and privacy-conserving enterprise platform, so you can focus on your prompts and chains. To access the hosted LLMs, contact us [here](https://www.predictionguard.com/getting-started) to get an enterprise access token. You will need this access token to continue.

## 2. (Optional) Install the Python client

You can configure and use Prediction Guard using our Python client or via REST API directly. If you are wanting to use the Python client, you can install it as follows:

```sh
$ pip install predictionguard
```

# 3. Start using one of our LLMs!

Suppose you want to prompt an LLM to answer a user query from a chat application. You can setup a message thread, which includes a system prompt (that instructs the LLM how to behave in responding) as follows:

```
[
    {
        "role": "system",
        "content": "You are a helpful assistant. Your model is hosted by Prediction Guard, a leading AI company."
    },
    {
        "role": "user",
        "content": "Where can I access the LLMs in a safe and secure environment?"
    }
]
```

You can then use our Python client or REST API to prompt one of our LLMs!

<CodeBlocks>
    <CodeBlock title="Python">
    ```python filename="main.py"
    import os
    import json
from predictionguard import PredictionGuard

# Set your Prediction Guard token as an environmental variable.
os.environ["PREDICTIONGUARD_API_KEY"] = "<api key>"

client = PredictionGuard()

# Define our prompt.

messages = [
{
"role": "system",
"content": "You are a helpful assistant. Your model is hosted by Prediction Guard, a leading AI company."
},
{
"role": "user",
"content": "Where can I access the LLMs in a safe and secure environment?"
}
]

result = client.chat.completions.create(
model="Neural-Chat-7B",
messages=messages
)

print(json.dumps(
result,
sort_keys=True,
indent=4,
separators=(',', ': ')
))

````
</CodeBlock>
<CodeBlock title="Go">
```go filename="prediction.go"
package main

import (
  "fmt"
  "strings"
  "net/http"
  "io/ioutil"
)

func main() {

  url := "https://api.predictionguard.com/chat/completions"
  method := "POST"

  payload := strings.NewReader(`{
    "model": "Neural-Chat-7B",
    "messages": [
        {
            "role": "system",
            "content": "You are a helpful assistant. Your model is hosted by Prediction Guard, a leading AI company."
        },
        {
            "role": "user",
            "content": "Where can I access the LLMs in a safe and secure environment?"
        }
    ]
}`)

  client := &http.Client {
  }
  req, err := http.NewRequest(method, url, payload)

  if err != nil {
    fmt.Println(err)
    return
  }
  req.Header.Add("Content-Type", "application/json")
  req.Header.Add("x-api-key", "<api key>")

  res, err := client.Do(req)
  if err != nil {
    fmt.Println(err)
    return
  }
  defer res.Body.Close()

  body, err := ioutil.ReadAll(res.Body)
  if err != nil {
    fmt.Println(err)
    return
  }
  fmt.Println(string(body))
}
````

</CodeBlock>
<CodeBlock title="NodeJS">
```js
const axios = require('axios');
let data = JSON.stringify({
  "model": "Neural-Chat-7B",
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful assistant. Your model is hosted by Prediction Guard, a leading AI company."
    },
    {
      "role": "user",
      "content": "Where can I access the LLMs in a safe and secure environment?"
    }
  ]
});

let config = {
method: 'post',
maxBodyLength: Infinity,
url: 'https://api.predictionguard.com/chat/completions',
headers: {
'Content-Type': 'application/json',
'x-api-key': '<api key>'
},
data : data
};

axios.request(config)
.then((response) => {
console.log(JSON.stringify(response.data));
})
.catch((error) => {
console.log(error);
});

````
</CodeBlock>
<CodeBlock title="cURL">
```bash
$ curl --location 'https://api.predictionguard.com/chat/completions' \
--header 'Content-Type: application/json' \
--header 'x-api-key: <api key>' \
--data '{
    "model": "Neural-Chat-7B",
    "messages": [
        {
            "role": "system",
            "content": "You are a helpful assistant. Your model is hosted by Prediction Guard, a leading AI company."
        },
        {
            "role": "user",
            "content": "Where can I access the LLMs in a safe and secure environment?"
        }
    ]
}'
````

</CodeBlock>
</CodeBlocks>
<Callout type="info" emoji="ℹ️">
  Note, you will need to replace `<your access token>` in the above examples with your actual access token.
</Callout>
