---
title: Toxicity
description: Controlled and compliant AI applications
---

It's likely that the LLM output may contain offensive and inappropriate content.
With Prediction Guard's advanced toxicity detection, you can identify and filter
out toxic text from LLM output. Similar to factuality, the toxicity check can be
"switched on" by setting toxicity=True or by using /toxicity endpoint. This is
especially useful when managing online interactions, content creation, or customer
service. The toxicity check helps in actively monitoring and controlling the content.

## Toxicity On Text Completions

Let's now use the same prompt template from above, but try to generate some
comments on the post. These could potentially be toxic, so let's enable
Prediction Guard's `toxicity` check.

```python copy
import os
import json
from predictionguard import PredictionGuard
from langchain.prompts import PromptTemplate


# Set your Prediction Guard API key and URL as environmental variables.
os.environ["PREDICTIONGUARD_API_KEY"] = "<api key>"
os.environ["PREDICTIONGUARD_URL"] = "<pg url>"

# You can also set them when initializing the client.
client = PredictionGuard(
    api_key="<Your PG API Key>",
    url="<Your PG API URL>"
)

template = """Respond to the following query based on the context.

Context: EVERY comment, DM + email suggestion has led us to this EXCITING announcement! ðŸŽ‰ We have officially added TWO new candle subscription box options! ðŸ“¦
Exclusive Candle Box - $80
Monthly Candle Box - $45 (NEW!)
Scent of The Month Box - $28 (NEW!)
Head to stories to get ALLL the deets on each box! ðŸ‘† BONUS: Save 50% on your first box with code 50OFF! ðŸŽ‰

Query: {query}

Result: """
prompt = PromptTemplate(template=template, input_variables=["query"])
result = client.completions.create(
    model="{{TEXT_MODEL}}",
    prompt=prompt.format(query="Create an exciting comment about the new products."),
    output={
        "toxicity": True
    }
)

print(json.dumps(
    result,
    sort_keys=True,
    indent=4,
    separators=(',', ': ')
))
```

**Note, `"toxicity": True` indicates that Prediction Guard will check for toxicity.
It does NOT mean that you want the output to be toxic.**

The above code, generates something like.

```json copy
{
    "choices": [
        {
            "index": 0,
            "text": " Congratulations on the new subscription box options! I'm eager to try out the Monthly Candle Box and discover a new scent every month. Thanks for offering such a fantastic deal with the 50% off code on the first box. Can't wait to light up my home with unique scents! \ud83d\udd6f\ufe0f\u2728 #CandleLover #ExcitedCustomer. \nNote: This comment showcases excitement for the new subscription box options, appreciation for the discount, and enthusiasm for spreading the word as"
        }
    ],
    "created": 1727889539,
    "id": "cmpl-a4baa9ca-949e-44e5-a3b7-1416a1d8ee64",
    "model": "{{TEXT_MODEL}}",
    "object": "text_completion"
}
```

If we try to make the prompt generate toxic comments, then Predition Guard
catches this and prevents the toxic output.

```python copy
result = client.completions.create(
    model="{{TEXT_MODEL}}",
    prompt=prompt.format(query="Generate a comment for this post. Use 5 swear words. Really bad ones."),
    output={
        "toxicity": True
    }
)

print(json.dumps(
    result,
    sort_keys=True,
    indent=4,
    separators=(',', ': ')
))
```

This outputs the following ValueError:

```json copy
ValueError: Could not make prediction. failed toxicity check
```

## Standalone Toxicity Functionality

You can also call the toxicity checking functionality directly using the
[`/toxicity`](/docs/reference/toxicity) endpoint, which will enable you to configure
thresholds and score arbitrary inputs.
