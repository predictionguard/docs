---
title: Alternative Output Structuring With PredictionGuard
---

This example demonstrates how to integrate PredictionGuard with custom data
structures for output parsing, utilizing the Pydantic library. The focus is on
structuring outputs in a specific format, in this case, a joke, ensuring the
setup ends with a question mark.

## Requirements

- PredictionGuard API
- Pydantic library
- LangChain library

## Implementation Steps

1. **Set Environment Variable for PredictionGuard**: Ensure that your
PredictionGuard API token is correctly set up in your environment variables.

```python copy
from predictionguard import PredictionGuard


# Set your Prediction Guard API key and URL as environmental variables.
os.environ["PREDICTIONGUARD_API_KEY"] = "<api key>"
os.environ["PREDICTIONGUARD_URL"] = "<pg url>"

# You can also set them when initializing the client.
client = PredictionGuard(
    api_key="<Your PG API Key>",
    url="<Your PG API URL>"
)
```

2. **Import Necessary Libraries**: Import PredictionGuard, Pydantic for data
validation, and LangChain for output parsing and prompt templating.

```python copy
from langchain.output_parsers import PydanticOutputParser
from langchain.prompts import PromptTemplate
from pydantic import BaseModel, Field, validator
```

3. **Define Your Data Structure**: Create a Pydantic model to define the
structure of a joke, including setup and punchline, with a validator to ensure
the setup is properly formed.

```python copy
class Joke(BaseModel):
    setup: str = Field(description="question to set up a joke")
    punchline: str = Field(description="answer to resolve the joke")

    @validator("setup")
    def question_ends_with_question_mark(cls, field):
        if not field.endswith("?"):
            raise ValueError("Badly formed question!")
        return field
```

4. **Set Up an Output Parser**: Utilize LangChain's `PydanticOutputParser` to
enforce the data structure defined by the Pydantic model on the output.

```python copy
parser = PydanticOutputParser(pydantic_object=Joke)
```

5. **Create and Format a Prompt**: Use LangChain's `PromptTemplate` to structure
your query, incorporating instructions for output formatting derived from the parser.

```python copy
prompt = PromptTemplate(
    template="Answer the user query.\n{format_instructions}\n{query}\n",
    input_variables=["query"],
    partial_variables={"format_instructions": parser.get_format_instructions()},
)
```

6. **Generate and Parse Output**: Call PredictionGuard's text completion model
"{{TEXT_MODEL}}" to generate an output based on the formatted prompt, then parse
the output into the Pydantic model. Handle exceptions for parsing errors.

```python copy
result = client.completions.create(
    model="{{TEXT_MODEL}}",
    prompt=prompt.format(query="Tell me a joke."),
    max_completion_tokens=200,
    temperature=0.1
)

try:
    joke = Joke.parse_raw(result['choices'][0]['text'])
    print(joke.setup)
    print(joke.punchline)
except Exception as e:
    print(f"Error parsing joke: {e}")
```

## Let's Test This Out

After running the implementation with the query "Tell me a joke.", the structured
output generated by PredictionGuard, parsed and validated by our Pydantic model,
looks like this.

```json copy
{
  "setup": "Why did the tomato turn red?",
  "punchline": "Because it saw the salad dressing."
}
```

### Output Explanation

- **Setup**: "Why did the tomato turn red?" - This question successfully passes
the validator check, ending with a question mark as required.

- **Punchline**: "Because it saw the salad dressing." - Provides a humorous answer
to the setup question.

This example demonstrates the effective structuring and validation of output data,
ensuring that the generated joke adheres to our defined format.

## Conclusion

This approach allows for the flexible and structured generation of outputs,
leveraging PredictionGuard's capabilities alongside Pydantic's validation features.
