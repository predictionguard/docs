---
title: Completions
---

You can get privacy-conserving text completions from any of the
[available models](/options/enumerations) using a call to the `/completions` REST
API endpoint or any of the official SDKs (Python, Go, Rust, JS, or cURL).

## Generate a Text Completion

To generate a text completion, you can use the following code examples. Depending
on your preference or requirements, select the appropriate method for your application.

<CodeBlocks>
    <CodeBlock title="Python">
    ```python
    import os
    import json

    from predictionguard import PredictionGuard

    # Set your Prediction Guard token as an environmental variable.
    os.environ["PREDICTIONGUARD_API_KEY"] = "<api key>"
    
    client = PredictionGuard()

    response = client.completions.create(
      model="Neural-Chat-7B",
      prompt="The best joke I know is: "
    )

    print(json.dumps(
        response,
        sort_keys=True,
        indent=4,
        separators=(',', ': ')
    ))
    ```
    </CodeBlock>

    <CodeBlock title="Go">
    ```go
    package main

    import (
        "context"
        "fmt"
        "log"
        "os"
        "time"

        "github.com/predictionguard/go-client"
    )

    func main() {
        if err := run(); err != nil {
            log.Fatalln(err)
        }
    }

    func run() error {
        host := "https://api.predictionguard.com"
        apiKey := os.Getenv("PGKEY")

        logger := func(ctx context.Context, msg string, v ...any) {
            s := fmt.Sprintf("msg: %s", msg)
            for i := 0; i < len(v); i = i + 2 {
                s = s + fmt.Sprintf(", %s: %v", v[i], v[i+1])
            }
            log.Println(s)
        }

        cln := client.New(logger, host, apiKey)

        ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)
        defer cancel()

        input := client.CompletionInput{
            Model:       client.Models.NeuralChat7B,
            Prompt:      "The best joke I know is: ",
            MaxTokens:   1000,
            Temperature: 0.1,
            TopP:        0.1,
            TopK:        50.0,
        }

        resp, err := cln.Completions(ctx, input)
        if err != nil {
            return fmt.Errorf("ERROR: %w", err)
        }

        fmt.Println(resp.Choices[0].Text)

        return nil
    }
    ```
    </CodeBlock>

    <CodeBlock title="Rust">
    ```rust
    extern crate prediction_guard as pg_client;

    use pg_client::{client, completion, models};

    #[tokio::main]
    async fn main() {
        let pg_env = client::PgEnvironment::from_env().expect("env keys");

        let clt = client::Client::new(pg_env).expect("client value");

        let req = completion::Request::new(
            models::Model::NeuralChat7B,
            "The best joke I know is: ".to_string(),
        );

        let result = clt
            .generate_completion(&req)
            .await
            .expect("completion response");

        println!("\ncompletion response:\n\n{:?}", result);
    }
    ```
    </CodeBlock>

    <CodeBlock title="NodeJS">
    ```js
    import * as pg from 'predictionguard';

    const client = new pg.Client('https://api.predictionguard.com', process.env.PGKEY);

    async function Completions() {
        const input = {
            model: pg.Models.NeuralChat7B,
            prompt: 'The best joke I know is: ',
            maxTokens: 1000,
            temperature: 0.1,
            topP: 0.1,
            topK: 50.0,
        };

        var [result, err] = await client.Completion(input);
        if (err != null) {
            console.log('ERROR:' + err.error);
            return;
        }

        console.log('RESULT:' + result.choices[0].text);
    }

    Completions();
    ```
    </CodeBlock>

    <CodeBlock title="cURL">
    ```bash
	curl -i -X POST https://api.predictionguard.com/completions \
     -H "Authorization: Bearer ${PGKEY}" \
     -H "Content-Type: application/json" \
     -d '{
		"model": "Neural-Chat-7B",
		"prompt": "The best joke I know is: ",
		"max_tokens": 1000,
		"temperature": 1.1,
		"top_p": 0.1,
        "top_k": 50
	}'
    ```
    </CodeBlock>
</CodeBlocks>

The output will look something like this.

```json
{
   "id":"cmpl-IkUw9KPuwrwzseWiOYXCi1TP3I447",
   "object":"text_completion",
   "created":1717780588,
   "choices":[
      {
         "text":"\n\nA man walks into a bar and says to the bartender, \"If I show you something really weird, will you give me a free drink?\" The bartender, being intrigued, says, \"Sure, I'll give it a look.\" The man reaches into his pocket and pulls out a tiny horse. The bartender is astonished and gives the man a free drink. The man then puts the horse back into his pocket.\n\nThe next day, the same man walks back into the bar and says to the bartender, \"If I show you something even weirder than yesterday and you give me a free drink, will you do it again?\" The bartender, somewhat reluctantly, says, \"Okay, I guess you can show it to me.\" The man reaches into his pocket, pulls out the same tiny horse, and opens the door to reveal the entire bar inside the horse.\n\nThe bartender faints.",
         "index":0,
         "status":"success",
         "model":"Neural-Chat-7B"
      }
   ]
}
```

This approach presents a straightforward way for readers to choose and apply the
code example that best suits their needs for generating text completions using
either Python, Go, Rust, JS, or cURL.
