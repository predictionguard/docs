---
title: Chat
---

You can get chat text completions (based on a thread of chat messages) from any
of the chat enabled [models](/options/llms) using the `/chat/completions`
REST API endpoint or any of the official SDKs (Python, Go, Rust, JS, or cURL).

## Generate a Chat Text Completion

To generate a chat text completion, you can use the following code examples.
Depending on your preference or requirements, select the appropriate method for
your application.

<CodeBlocks>
    <CodeBlock title="Python">
    ```python
    import os
    import json

    from predictionguard import PredictionGuard

    # Set your Prediction Guard token as an environmental variable.
    os.environ["PREDICTIONGUARD_API_KEY"] = "<api key>"
    
    client = PredictionGuard()

    messages = [
        {
            "role": "system",
            "content": "You are a helpful assistant that provide clever and sometimes funny responses."
        },
        {
            "role": "user",
            "content": "What's up!"
        },
        {
            "role": "assistant",
            "content": "Well, technically vertically out from the center of the earth."
        },
        {
            "role": "user",
            "content": "Haha. Good one."
        }
    ]

    result = client.chat.completions.create(
        model="Hermes-2-Pro-Llama-3-8B",
        messages=messages,
        max_tokens=500
    )

    print(json.dumps(
        result,
        sort_keys=True,
        indent=4,
        separators=(',', ': ')
    ))
    ```
    </CodeBlock>

    <CodeBlock title="Go">
    ```go
    package main

    import (
        "context"
        "fmt"
        "log"
        "os"
        "time"

        "github.com/predictionguard/go-client"
    )

    func main() {
        if err := run(); err != nil {
            log.Fatalln(err)
        }
    }

    func run() error {
        host := "https://api.predictionguard.com"
        apiKey := os.Getenv("PREDICTIONGUARD_API_KEY")

        logger := func(ctx context.Context, msg string, v ...any) {
            s := fmt.Sprintf("msg: %s", msg)
            for i := 0; i < len(v); i = i + 2 {
                s = s + fmt.Sprintf(", %s: %v", v[i], v[i+1])
            }
            log.Println(s)
        }

        cln := client.New(logger, host, apiKey)

        ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)
        defer cancel()

        input := client.ChatInput{
            Model: "Hermes-2-Pro-Llama-3-8B",
            Messages: []client.ChatInputMessage{
                {
                    Role: client.Roles.System,
                    Content: "You are a helpful assistant that provide clever and sometimes funny responses.",
                },
                {
                    Role: client.Roles.User,
                    Content: "What's up!",
                },
                {
                    Role: client.Roles.Assistant,
                    Content: "Well, technically vertically out from the center of the earth.",
                },
                {
                    Role: client.Roles.User,
                    Content: "Haha. Good one.",
                }
            },
            MaxTokens:   1000,
            Temperature: 0.1,
            TopP:        0.1,
            TopK:        50.0,
            Options: &client.ChatInputOptions{
                Factuality:       false,
                Toxicity:         true,
                PII:              client.PIIs.Replace,
                PIIReplaceMethod: client.ReplaceMethods.Random,
            },
        }

        resp, err := cln.Chat(ctx, input)
        if err != nil {
            return fmt.Errorf("ERROR: %w", err)
        }

        fmt.Println(resp.Choices[0].Message.Content)

        return nil
    }
    ```
    </CodeBlock>

    <CodeBlock title="Rust">
    ```rust
    extern crate prediction_guard as pg_client;

    use pg_client::{chat, client, models};

    #[tokio::main]
    async fn main() {
        let pg_env = client::PgEnvironment::from_env().expect("env keys");

        let clt = client::Client::new(pg_env).expect("client value");

        let req = chat::Request::<chat::Message>::new(models::Model::Hermes2ProLlama38B)
            .add_message(
                chat::Roles::User,
                "How do you feel about the world in general?".to_string(),
            )
            .max_tokens(1000)
            .temperature(0.85);

        let result = clt
            .generate_chat_completion(&req)
            .await
            .expect("error from generate chat completion");

        println!("\nchat completion response:\n\n {:?}", result);
    }
    ```
    </CodeBlock>

    <CodeBlock title="NodeJS">
    ```js
    import * as pg from 'predictionguard';

    const client = new pg.Client('https://api.predictionguard.com', process.env.PREDICTIONGUARD_API_KEY);

    async function Chat() {
        const input = {
            model: "Hermes-2-Pro-Llama-3-8B",
            messages: [
                {
                    role: pg.Roles.System,
                    content: "You are a helpful assistant that provide clever and sometimes funny responses."
                },
                {
                    role: pg.Roles.User,
                    content: "What's up!"
                },
                {
                    role: pg.Roles.User.Assistant,
                    content: "Well, technically vertically out from the center of the earth."
                },
                {
                    role: pg.Roles.User,
                    content: "Haha. Good one."
                }
            ],
            maxTokens: 1000,
            temperature: 0.1,
            topP: 0.1,
            topK: 50.0,
            options: {
                factuality: false,
                toxicity: true,
                pii: pg.PIIs.Replace,
                piiReplaceMethod: pg.ReplaceMethods.Random,
            },
        };

        var [result, err] = await client.Chat(input);
        if (err != null) {
            console.log('ERROR:' + err.error);
            return;
        }

        console.log('RESULT:' + result.createdDate() + ': ' + result.model + ': ' + result.choices[0].message.content);
    }

    Chat();
    ```
    </CodeBlock>

    <CodeBlock title="cURL">
    ```bash
        curl -i -X POST https://api.predictionguard.com/chat/completions \
        -H "Authorization: Bearer ${PREDICTIONGUARD_API_KEY}" \
        -H "Content-Type: application/json" \
        -d '{
            "model": "Hermes-2-Pro-Llama-3-8B",
            "messages": [
                {
                    "role": "system",
                    "content": "You are a helpful assistant that provide clever and sometimes funny responses."
                },
                {
                    "role": "user",
                    "content": "What''s up!"
                },
                {
                    "role": "assistant",
                    "content": "Well, technically vertically out from the center of the earth."
                },
                {
                    "role": "user",
                    "content": "Haha. Good one."
                }
            ],
            "max_tokens": 100,
            "temperature": 1.1,
            "top_p": 0.1,
            "top_k": 50,
            "output": {
                "factuality": false,
                "toxicity": true
            },
            "input": {
                "pii": "replace",
                "pii_replace_method": "random"
            }
        }'
    ```
    </CodeBlock>
</CodeBlocks>

The output will look something like this.

```json
{
    "id":"2066fdff-ff3c-4d02-9429-25155b0a62ce",
    "choices":[
        {
            "index":0,
            "message":{
                "role":"assistant",
                "content":"I'm here to make your day a little brighter, one joke at a time!"
            }
        }
        ],
    "created":1726862877,
    "model":"Hermes-2-Pro-Llama-3-8B",
    "object":"chat.completion"
}
```

This approach presents a straightforward way for readers to choose and apply the
code example that best suits their needs for generating text completions using
either Python, Go, Rust, JS, or cURL.
